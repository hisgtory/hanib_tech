# Episode 10-4: ìì—°ì–´ ì²˜ë¦¬, AIê°€ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë²•

## ğŸ¬ Scene: ì±—ë´‡ì´ ì´í•´í•œ ìˆœê°„

```
ê³ ê° ì„œë¹„ìŠ¤ ê°œì„  í”„ë¡œì íŠ¸

Before (ê·œì¹™ ê¸°ë°˜ ì±—ë´‡):
ê³ ê°: "ë°°ì†¡ì´ ë„ˆë¬´ ëŠë ¤ì„œ í™”ê°€ ë‚˜ìš”"
ì±—ë´‡: "ë°°ì†¡ì— ëŒ€í•´ ë¬¸ì˜í•˜ì…¨êµ°ìš”. ë°°ì†¡ ì¡°íšŒëŠ”..."
ê³ ê°: "ì•„ë‹ˆ, ë¶ˆë§Œì„ ë§í•˜ëŠ” ê±°ì˜ˆìš”!"
ì±—ë´‡: "ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

After (NLP ì±—ë´‡):
ê³ ê°: "ë°°ì†¡ì´ ë„ˆë¬´ ëŠë ¤ì„œ í™”ê°€ ë‚˜ìš”"
ì±—ë´‡: "ë°°ì†¡ ì§€ì—°ìœ¼ë¡œ ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤ ğŸ˜”
       ì£¼ë¬¸ë²ˆí˜¸ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ì¦‰ì‹œ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
       ë³´ìƒìœ¼ë¡œ ë‹¤ìŒ ë°°ì†¡ë¹„ë¥¼ ë¬´ë£Œë¡œ í•´ë“œë¦´ê²Œìš”."

ì°¨ì´ì :
- ê°ì • ì¸ì‹: í™”ë‚¨ (ë¶€ì • 0.92)
- ì˜ë„ íŒŒì•…: ë¶ˆë§Œ ì œê¸°
- í•µì‹¬ ì¶”ì¶œ: ë°°ì†¡ ì§€ì—°
- ì ì ˆí•œ ëŒ€ì‘: ì‚¬ê³¼ + í•´ê²°ì±…

ê³ ê° ë§Œì¡±ë„: 45% â†’ 87%
```

**NLPëŠ” í…ìŠ¤íŠ¸ë¥¼ ì´í•´ê°€ ì•„ë‹Œ ê³„ì‚° ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë°”ê¾¸ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.**

## Part 1: NLP ê¸°ì´ˆ

### ğŸ“ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

```python
# NLP íŒŒì´í”„ë¼ì¸ ì‹œì‘

import nltk
import spacy
from konlpy.tag import Okt  # í•œêµ­ì–´

class TextPreprocessor:
    def __init__(self, language='ko'):
        if language == 'ko':
            self.tokenizer = Okt()
        else:
            self.nlp = spacy.load('en_core_web_sm')
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # 1. ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # 2. íŠ¹ìˆ˜ë¬¸ì ì œê±°
        import re
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
        
        # 3. ê³µë°± ì •ê·œí™”
        text = ' '.join(text.split())
        
        return text
    
    def tokenize(self, text):
        """í† í°í™”"""
        # í•œêµ­ì–´
        if hasattr(self, 'tokenizer'):
            tokens = self.tokenizer.morphs(text)
            # "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤" â†’ ["ë‚˜", "ëŠ”", "í•™êµ", "ì—", "ê°€", "ã„´ë‹¤"]
        
        # ì˜ì–´
        else:
            doc = self.nlp(text)
            tokens = [token.text for token in doc]
            # "I am going to school" â†’ ["I", "am", "going", "to", "school"]
        
        return tokens
    
    def remove_stopwords(self, tokens):
        """ë¶ˆìš©ì–´ ì œê±°"""
        stopwords_ko = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—']
        stopwords_en = ['the', 'is', 'at', 'which', 'on']
        
        filtered = [t for t in tokens if t not in stopwords_ko + stopwords_en]
        return filtered
    
    def lemmatize(self, tokens):
        """ì›í˜• ë³µì›"""
        # going â†’ go, went â†’ go, better â†’ good
        if hasattr(self, 'nlp'):
            doc = self.nlp(' '.join(tokens))
            return [token.lemma_ for token in doc]
        return tokens
    
    def pos_tagging(self, text):
        """í’ˆì‚¬ íƒœê¹…"""
        if hasattr(self, 'tokenizer'):
            pos = self.tokenizer.pos(text)
            # [('ë‚˜', 'Noun'), ('ëŠ”', 'Josa'), ('í•™êµ', 'Noun')]
        else:
            doc = self.nlp(text)
            pos = [(token.text, token.pos_) for token in doc]
            # [('I', 'PRON'), ('am', 'AUX'), ('going', 'VERB')]
        
        return pos

# ì‚¬ìš© ì˜ˆì‹œ
processor = TextPreprocessor('ko')
text = "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”! ê³µì›ì— ê°€ê³  ì‹¶ì–´ìš”."
tokens = processor.tokenize(processor.clean_text(text))
print(tokens)
# ['ì˜¤ëŠ˜', 'ë‚ ì”¨', 'ê°€', 'ì •ë§', 'ì¢‹', 'ë„¤ìš”', 'ê³µì›', 'ì—', 'ê°€', 'ê³ ', 'ì‹¶', 'ì–´ìš”']
```

### ğŸ”¢ í…ìŠ¤íŠ¸ ë²¡í„°í™”

```python
# í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜

# 1. Bag of Words (BoW)
from sklearn.feature_extraction.text import CountVectorizer

texts = [
    "ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ì¢‹ì•„í•œë‹¤",
    "ë‚˜ëŠ” ë°”ë‚˜ë‚˜ë¥¼ ì¢‹ì•„í•œë‹¤",
    "ì‚¬ê³¼ëŠ” ë¹¨ê°„ìƒ‰ì´ë‹¤"
]

vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(texts)
print(vectorizer.get_feature_names_out())
# ['ë‚˜ëŠ”', 'ë°”ë‚˜ë‚˜ë¥¼', 'ë¹¨ê°„ìƒ‰ì´ë‹¤', 'ì‚¬ê³¼ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ì¢‹ì•„í•œë‹¤']
print(bow_matrix.toarray())
# [[1, 0, 0, 0, 1, 1],  # ì²« ë²ˆì§¸ ë¬¸ì¥
#  [1, 1, 0, 0, 0, 1],  # ë‘ ë²ˆì§¸ ë¬¸ì¥
#  [0, 0, 1, 1, 0, 0]]  # ì„¸ ë²ˆì§¸ ë¬¸ì¥

# 2. TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(texts)
# ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ê°€ì¤‘ì¹˜ ë‚®ê²Œ, íŠ¹ë³„í•œ ë‹¨ì–´ëŠ” ë†’ê²Œ

# 3. Word2Vec
from gensim.models import Word2Vec

sentences = [text.split() for text in texts]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# ë‹¨ì–´ ë²¡í„°
vector = model.wv['ì‚¬ê³¼']  # 100ì°¨ì› ë²¡í„°
similar = model.wv.most_similar('ì‚¬ê³¼', topn=3)
# [('ë°”ë‚˜ë‚˜', 0.92), ('ê³¼ì¼', 0.88), ('ë¹¨ê°„ìƒ‰', 0.75)]

# 4. ì‚¬ì „í•™ìŠµ ì„ë² ë”©
import torch
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')

inputs = tokenizer("ì•ˆë…•í•˜ì„¸ìš”", return_tensors='pt')
outputs = model(**inputs)
embeddings = outputs.last_hidden_state  # 768ì°¨ì›
```

## Part 2: ì£¼ìš” NLP íƒœìŠ¤í¬

### ğŸ’­ ê°ì • ë¶„ì„ (Sentiment Analysis)

```python
# ê°ì • ë¶„ì„ êµ¬í˜„

class SentimentAnalyzer:
    def __init__(self):
        from transformers import pipeline
        self.analyzer = pipeline(
            "sentiment-analysis",
            model="nlptown/bert-base-multilingual-uncased-sentiment"
        )
    
    def analyze(self, text):
        """ê°ì • ë¶„ì„"""
        result = self.analyzer(text)[0]
        
        # ì„¸ë¶€ ê°ì • ë¶„ë¥˜
        emotions = self.detect_emotions(text)
        
        return {
            'sentiment': result['label'],
            'confidence': result['score'],
            'emotions': emotions
        }
    
    def detect_emotions(self, text):
        """ì„¸ë¶€ ê°ì • ê°ì§€"""
        emotion_words = {
            'joy': ['í–‰ë³µ', 'ê¸°ì¨', 'ì¢‹ì•„', 'ìµœê³ ', 'ì‚¬ë‘'],
            'anger': ['í™”ë‚¨', 'ì§œì¦', 'ì‹«ì–´', 'ìµœì•…', 'ë¶„ë…¸'],
            'sadness': ['ìŠ¬í””', 'ìš°ìš¸', 'ì™¸ë¡œì›€', 'ëˆˆë¬¼'],
            'fear': ['ë¬´ì„œì›€', 'ë‘ë ¤ì›€', 'ê±±ì •', 'ë¶ˆì•ˆ'],
            'surprise': ['ë†€ë¼ì›€', 'ê¹œì§', 'ëŒ€ë°•', 'í—']
        }
        
        detected = {}
        for emotion, words in emotion_words.items():
            score = sum(1 for word in words if word in text)
            if score > 0:
                detected[emotion] = score / len(words)
        
        return detected

# ë¦¬ë·° ê°ì • ë¶„ì„
analyzer = SentimentAnalyzer()

reviews = [
    "ì´ ì œí’ˆ ì •ë§ ìµœê³ ì˜ˆìš”! ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤!",
    "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ê³  í’ˆì§ˆë„ ë³„ë¡œë„¤ìš”",
    "ê·¸ëƒ¥ ê·¸ë˜ìš”. ì“¸ë§Œí•œ ì •ë„?"
]

for review in reviews:
    result = analyzer.analyze(review)
    print(f"{review[:20]}... -> {result['sentiment']} ({result['confidence']:.2f})")
```

### ğŸ¯ ê°œì²´ëª… ì¸ì‹ (NER)

```python
# Named Entity Recognition

class NERExtractor:
    def __init__(self):
        import spacy
        self.nlp = spacy.load("ko_core_news_sm")  # í•œêµ­ì–´ ëª¨ë¸
    
    def extract_entities(self, text):
        """ê°œì²´ëª… ì¶”ì¶œ"""
        doc = self.nlp(text)
        
        entities = {
            'PERSON': [],
            'ORG': [],
            'LOC': [],
            'DATE': [],
            'MONEY': [],
            'PRODUCT': []
        }
        
        for ent in doc.ents:
            if ent.label_ in entities:
                entities[ent.label_].append(ent.text)
        
        return entities
    
    def extract_custom_entities(self, text):
        """ì»¤ìŠ¤í…€ ê°œì²´ ì¶”ì¶œ"""
        import re
        
        # ì´ë©”ì¼
        emails = re.findall(r'[\w\.-]+@[\w\.-]+', text)
        
        # ì „í™”ë²ˆí˜¸
        phones = re.findall(r'\d{3}-\d{4}-\d{4}', text)
        
        # ì£¼ë¬¸ë²ˆí˜¸
        orders = re.findall(r'ORD\d{10}', text)
        
        return {
            'emails': emails,
            'phones': phones,
            'orders': orders
        }

# ì‚¬ìš© ì˜ˆì‹œ
ner = NERExtractor()
text = """
ì‚¼ì„±ì „ìì˜ ì´ì¬ìš© ë¶€íšŒì¥ì´ 2024ë…„ 3ì›” 15ì¼ 
ì„œìš¸ ê°•ë‚¨êµ¬ì—ì„œ ì—´ë¦° í–‰ì‚¬ì— ì°¸ì„í–ˆìŠµë‹ˆë‹¤.
ë¬¸ì˜: support@samsung.com, 010-1234-5678
"""

entities = ner.extract_entities(text)
print(entities)
# {'PERSON': ['ì´ì¬ìš©'], 'ORG': ['ì‚¼ì„±ì „ì'], 'LOC': ['ì„œìš¸', 'ê°•ë‚¨êµ¬'], 
#  'DATE': ['2024ë…„ 3ì›” 15ì¼'], 'MONEY': [], 'PRODUCT': []}
```

### ğŸ’¬ ì˜ë„ ë¶„ë¥˜ (Intent Classification)

```python
# ì±—ë´‡ ì˜ë„ ë¶„ë¥˜

class IntentClassifier:
    def __init__(self):
        self.intents = {
            'greeting': ['ì•ˆë…•', 'í•˜ì´', 'hello', 'ë°˜ê°€ì›Œ'],
            'goodbye': ['ì˜ê°€', 'ë°”ì´', 'ë‹¤ìŒì—', 'ì•ˆë…•íˆ'],
            'order_status': ['ì£¼ë¬¸', 'ë°°ì†¡', 'ì–¸ì œ', 'ë„ì°©'],
            'refund': ['í™˜ë¶ˆ', 'ë°˜í’ˆ', 'êµí™˜', 'ì·¨ì†Œ'],
            'complaint': ['ë¶ˆë§Œ', 'í™”ë‚¨', 'ì§œì¦', 'ìµœì•…'],
            'praise': ['ì¢‹ì•„', 'ìµœê³ ', 'ë§Œì¡±', 'ì¶”ì²œ']
        }
    
    def classify(self, text):
        """ì˜ë„ ë¶„ë¥˜"""
        text_lower = text.lower()
        scores = {}
        
        for intent, keywords in self.intents.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                scores[intent] = score
        
        if not scores:
            return 'unknown'
        
        return max(scores, key=scores.get)
    
    def extract_slots(self, text, intent):
        """ìŠ¬ë¡¯ ì¶”ì¶œ"""
        slots = {}
        
        if intent == 'order_status':
            # ì£¼ë¬¸ë²ˆí˜¸ ì¶”ì¶œ
            import re
            order_match = re.search(r'\d{10}', text)
            if order_match:
                slots['order_id'] = order_match.group()
        
        elif intent == 'refund':
            # ì œí’ˆëª… ì¶”ì¶œ
            # ì‹¤ì œë¡œëŠ” NERì´ë‚˜ ë” ë³µì¡í•œ ë¡œì§ í•„ìš”
            slots['product'] = self.extract_product(text)
        
        return slots

# ì±—ë´‡ ì‘ë‹µ ìƒì„±
def generate_response(intent, slots):
    responses = {
        'greeting': "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        'order_status': f"ì£¼ë¬¸ë²ˆí˜¸ {slots.get('order_id', '')}ì˜ ë°°ì†¡ ìƒíƒœë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.",
        'refund': "í™˜ë¶ˆ ì ˆì°¨ë¥¼ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        'complaint': "ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì„ ë§ì”€í•´ì£¼ì„¸ìš”.",
        'unknown': "ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”."
    }
    
    return responses.get(intent, responses['unknown'])
```

## Part 3: ê³ ê¸‰ NLP

### ğŸ¤– Question Answering

```python
# ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

from transformers import pipeline

class QASystem:
    def __init__(self):
        self.qa_pipeline = pipeline(
            "question-answering",
            model="klue/bert-base"  # í•œêµ­ì–´ ëª¨ë¸
        )
    
    def answer(self, context, question):
        """ë¬¸ì„œì—ì„œ ë‹µë³€ ì°¾ê¸°"""
        result = self.qa_pipeline(
            question=question,
            context=context
        )
        
        return {
            'answer': result['answer'],
            'confidence': result['score'],
            'start': result['start'],
            'end': result['end']
        }
    
    def multi_hop_qa(self, documents, question):
        """ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ë‹µë³€ ì°¾ê¸°"""
        candidates = []
        
        for doc in documents:
            result = self.answer(doc, question)
            candidates.append(result)
        
        # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ ì„ íƒ
        best = max(candidates, key=lambda x: x['confidence'])
        return best

# ì‚¬ìš© ì˜ˆì‹œ
qa = QASystem()
context = """
ì•„ì´í° 15ëŠ” 2023ë…„ 9ì›”ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.
ê°€ê²©ì€ 128GB ëª¨ë¸ ê¸°ì¤€ 125ë§Œì›ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
ìƒˆë¡œìš´ A17 Pro ì¹©ì…‹ì„ íƒ‘ì¬í–ˆìœ¼ë©°, 
USB-C í¬íŠ¸ë¥¼ ì²˜ìŒìœ¼ë¡œ ì±„íƒí–ˆìŠµë‹ˆë‹¤.
"""

questions = [
    "ì•„ì´í° 15ëŠ” ì–¸ì œ ì¶œì‹œë˜ì—ˆë‚˜ìš”?",
    "ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?",
    "ì–´ë–¤ í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?"
]

for q in questions:
    answer = qa.answer(context, q)
    print(f"Q: {q}")
    print(f"A: {answer['answer']} (ì‹ ë¢°ë„: {answer['confidence']:.2f})\n")
```

### ğŸ“ í…ìŠ¤íŠ¸ ìš”ì•½

```python
# ë¬¸ì„œ ìš”ì•½

class TextSummarizer:
    def __init__(self):
        from transformers import pipeline
        self.summarizer = pipeline(
            "summarization",
            model="gogamza/kobart-summarization"
        )
    
    def extractive_summary(self, text, num_sentences=3):
        """ì¶”ì¶œ ìš”ì•½: ì¤‘ìš”í•œ ë¬¸ì¥ ì„ íƒ"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        
        sentences = text.split('.')
        if len(sentences) <= num_sentences:
            return text
        
        # TF-IDFë¡œ ë¬¸ì¥ ì¤‘ìš”ë„ ê³„ì‚°
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)
        
        # ë¬¸ì¥ ì ìˆ˜ ê³„ì‚°
        scores = np.sum(tfidf_matrix.toarray(), axis=1)
        
        # ìƒìœ„ nê°œ ë¬¸ì¥ ì„ íƒ
        top_indices = np.argsort(scores)[-num_sentences:]
        top_indices.sort()
        
        summary = '. '.join([sentences[i] for i in top_indices])
        return summary + '.'
    
    def abstractive_summary(self, text, max_length=150):
        """ìƒì„± ìš”ì•½: ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„±"""
        result = self.summarizer(
            text,
            max_length=max_length,
            min_length=30,
            do_sample=False
        )
        
        return result[0]['summary_text']

# ê¸´ ë¬¸ì„œ ìš”ì•½
long_text = """
[ê¸´ ë‰´ìŠ¤ ê¸°ì‚¬ë‚˜ ë³´ê³ ì„œ ë‚´ìš©]
"""

summarizer = TextSummarizer()
summary = summarizer.abstractive_summary(long_text)
print("ìš”ì•½:", summary)
```

## Part 4: í•œêµ­ì–´ NLP

### ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹ìˆ˜ì„±

```python
# í•œêµ­ì–´ NLP ë„ì „ê³¼ì œ

class KoreanNLP:
    def __init__(self):
        from konlpy.tag import Okt, Komoran, Kkma
        self.okt = Okt()
        self.komoran = Komoran()
    
    def handle_spacing(self, text):
        """ë„ì–´ì“°ê¸° êµì •"""
        from pykospacing import Spacing
        spacing = Spacing()
        corrected = spacing(text)
        return corrected
        # "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤" â†’ "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤"
    
    def handle_typos(self, text):
        """ë§ì¶¤ë²• êµì •"""
        from hanspell import spell_checker
        result = spell_checker.check(text)
        return result.checked
        # "ë§ì¶¤ë»¡ì´ í‹€ë ¸ìŠµë‹ˆë‹¤" â†’ "ë§ì¶¤ë²•ì´ í‹€ë ¸ìŠµë‹ˆë‹¤"
    
    def extract_keywords(self, text):
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from krwordrank.word import KRWordRank
        
        wordrank = KRWordRank(
            min_count=2,
            max_length=10
        )
        
        keywords = wordrank.extract(text, beta=0.85, max_iter=10)
        return keywords
    
    def analyze_morphology(self, text):
        """í˜•íƒœì†Œ ë¶„ì„ ë¹„êµ"""
        results = {
            'Okt': self.okt.pos(text),
            'Komoran': self.komoran.pos(text)
        }
        
        return results
    
    def handle_neologism(self, text):
        """ì‹ ì¡°ì–´ ì²˜ë¦¬"""
        slang_dict = {
            'ã…‡ã…ˆ': 'ì¸ì •',
            'ã„¹ã…‡': 'ë¦¬ì–¼',
            'ê°‘ë¶„ì‹¸': 'ê°‘ìê¸° ë¶„ìœ„ê¸° ì‹¸í•´ì§',
            'ë³„ë‹¤ì¤„': 'ë³„ê±¸ ë‹¤ ì¤„ì¸ë‹¤'
        }
        
        for slang, meaning in slang_dict.items():
            text = text.replace(slang, meaning)
        
        return text

# í•œêµ­ì–´ ì²˜ë¦¬ ì˜ˆì‹œ
knlp = KoreanNLP()

text = "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤"
corrected = knlp.handle_spacing(text)
print(f"ì›ë¬¸: {text}")
print(f"êµì •: {corrected}")

# í˜•íƒœì†Œ ë¶„ì„ ì°¨ì´
text = "ë¡¯ë°ë§ˆíŠ¸ì—ì„œ ìš°ìœ ë¥¼ ìƒ€ë‹¤"
morphs = knlp.analyze_morphology(text)
for analyzer, result in morphs.items():
    print(f"{analyzer}: {result}")
```

## Part 5: ì‹¤ì „ ì‘ìš©

### ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì‘ìš©

```python
# ê³ ê° ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ

class ReviewAnalyzer:
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        self.ner_extractor = NERExtractor()
        
    def analyze_reviews(self, reviews):
        """ë¦¬ë·° ì¢…í•© ë¶„ì„"""
        results = {
            'total': len(reviews),
            'positive': 0,
            'negative': 0,
            'neutral': 0,
            'keywords': {},
            'issues': [],
            'features': {}
        }
        
        for review in reviews:
            # ê°ì • ë¶„ì„
            sentiment = self.sentiment_analyzer.analyze(review)
            if sentiment['confidence'] > 0.7:
                if sentiment['sentiment'] in ['4 stars', '5 stars']:
                    results['positive'] += 1
                elif sentiment['sentiment'] in ['1 star', '2 stars']:
                    results['negative'] += 1
                else:
                    results['neutral'] += 1
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            self.extract_keywords(review, results['keywords'])
            
            # ë¬¸ì œì  ì¶”ì¶œ
            if 'ë¬¸ì œ' in review or 'ë¶ˆí¸' in review:
                results['issues'].append(review[:50])
            
            # ê¸°ëŠ¥ë³„ í‰ê°€
            self.analyze_features(review, results['features'])
        
        return results
    
    def generate_insights(self, analysis):
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ê°ì • ë¶„í¬
        total = analysis['total']
        pos_ratio = analysis['positive'] / total
        
        if pos_ratio < 0.5:
            insights.append("âš ï¸ ë¶€ì • ë¦¬ë·°ê°€ 50% ì´ìƒì…ë‹ˆë‹¤. ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif pos_ratio > 0.8:
            insights.append("âœ… ê¸ì • ë¦¬ë·°ê°€ 80% ì´ìƒì…ë‹ˆë‹¤. í›Œë¥­í•©ë‹ˆë‹¤!")
        
        # ì£¼ìš” ì´ìŠˆ
        if analysis['issues']:
            top_issues = analysis['issues'][:3]
            insights.append(f"ì£¼ìš” ë¬¸ì œì : {', '.join(top_issues)}")
        
        # ê°œì„  ì œì•ˆ
        if 'ë°°ì†¡' in analysis['keywords'] and analysis['keywords']['ë°°ì†¡'] > 10:
            insights.append("ë°°ì†¡ ê´€ë ¨ ì–¸ê¸‰ì´ ë§ìŠµë‹ˆë‹¤. ë¬¼ë¥˜ ê°œì„ ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return insights

# ì‹¤ì œ ì‚¬ìš©
analyzer = ReviewAnalyzer()
reviews = load_reviews_from_db()
analysis = analyzer.analyze_reviews(reviews)
insights = analyzer.generate_insights(analysis)

print("=== ë¦¬ë·° ë¶„ì„ ë¦¬í¬íŠ¸ ===")
print(f"ì´ ë¦¬ë·°: {analysis['total']}")
print(f"ê¸ì •: {analysis['positive']} ({analysis['positive']/analysis['total']*100:.1f}%)")
print(f"ë¶€ì •: {analysis['negative']} ({analysis['negative']/analysis['total']*100:.1f}%)")
print("\nì¸ì‚¬ì´íŠ¸:")
for insight in insights:
    print(f"  {insight}")
```

## ğŸ Bonus: LLM í™œìš©

### ğŸš€ Large Language Models

```python
# GPT í™œìš© ì˜ˆì‹œ

import openai

class LLMAssistant:
    def __init__(self, api_key):
        openai.api_key = api_key
    
    def generate_product_description(self, product_info):
        """ì œí’ˆ ì„¤ëª… ìƒì„±"""
        prompt = f"""
        ì œí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ë ¥ì ì¸ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        ì œí’ˆëª…: {product_info['name']}
        ì¹´í…Œê³ ë¦¬: {product_info['category']}
        íŠ¹ì§•: {', '.join(product_info['features'])}
        íƒ€ê²Ÿ: {product_info['target']}
        
        í†¤: ì¹œê·¼í•˜ê³  ì‹ ë¢°ê° ìˆê²Œ
        ê¸¸ì´: 100-150ì
        """
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    def improve_ux_writing(self, original_text, context):
        """UX ë¼ì´íŒ… ê°œì„ """
        prompt = f"""
        ë‹¤ìŒ UI í…ìŠ¤íŠ¸ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:
        ì›ë¬¸: {original_text}
        ì»¨í…ìŠ¤íŠ¸: {context}
        
        ê°œì„  ë°©í–¥:
        - ë” ëª…í™•í•˜ê²Œ
        - ë” ì¹œê·¼í•˜ê²Œ
        - ë” ê°„ê²°í•˜ê²Œ
        """
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.choices[0].message.content

# í™œìš© ì˜ˆì‹œ
assistant = LLMAssistant(api_key="your-key")

product = {
    'name': 'ìŠ¤ë§ˆíŠ¸ ë¬´ë“œë“±',
    'category': 'ì¡°ëª…',
    'features': ['ìŒì„± ì œì–´', '1600ë§Œ ì»¬ëŸ¬', 'íƒ€ì´ë¨¸ ê¸°ëŠ¥'],
    'target': '20-30ëŒ€'
}

description = assistant.generate_product_description(product)
print("ìƒì„±ëœ ì„¤ëª…:", description)
```

## ğŸ’¡ í•µì‹¬ ë©”ì‹œì§€

> "NLPëŠ” ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼
> íŒ¨í„´ì„ ì°¾ì•„ ê³„ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
> 
> ì™„ë²½í•œ ì´í•´ëŠ” ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ,
> ì¶©ë¶„íˆ ìœ ìš©í•œ ê·¼ì‚¬ì¹˜ëŠ” ê°€ëŠ¥í•©ë‹ˆë‹¤.
> 
> ì–¸ì–´ì˜ ë³µì¡ì„±ì„ ìˆ˜í•™ìœ¼ë¡œ í’€ì–´ë‚´ëŠ” ê²ƒ,
> ê·¸ê²ƒì´ NLPì˜ ë§ˆë²•ì…ë‹ˆë‹¤."

**ê¸°ì–µí•˜ì„¸ìš”:**
- ì „ì²˜ë¦¬ê°€ ì„±ëŠ¥ì˜ ì ˆë°˜
- ì–¸ì–´ë³„ íŠ¹ì„± ê³ ë ¤
- ì»¨í…ìŠ¤íŠ¸ê°€ í•µì‹¬
- í‰ê°€ ì§€í‘œ ì„¤ì •

## ğŸš€ ë‹¤ìŒ ì—í”¼ì†Œë“œ ì˜ˆê³ 

**"Episode 11-1: ì• ìì¼ê³¼ ìŠ¤í¬ëŸ¼ ì‹¤ì „"**

ì§„ì§œ ì• ìì¼ ì´í•´í•˜ê¸°:
- ìŠ¤í”„ë¦°íŠ¸ í”Œë˜ë‹
- ë°ì¼ë¦¬ ìŠ¤í¬ëŸ¼
- íšŒê³ ì™€ ê°œì„ 
- ì¹¸ë°˜ vs ìŠ¤í¬ëŸ¼

"ì• ìì¼ì€ ë¬¸ì„œê°€ ì•„ë‹ˆë¼ ë¬¸í™”ì…ë‹ˆë‹¤"

---

*"Language is the dress of thought."
- Samuel Johnson*

**#NLP #ìì—°ì–´ì²˜ë¦¬ #ChatGPT #TextAnalysis #AI**