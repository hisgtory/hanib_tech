# Episode 10-3: ì»´í“¨í„° ë¹„ì „, ê¸°ê³„ê°€ ë³´ëŠ” ì„¸ìƒ

## ğŸ¬ Scene: ì–¼êµ´ì¸ì‹ ì¶œì… ì‹œìŠ¤í…œ

```
ìƒˆ ì˜¤í”¼ìŠ¤ ì²«ë‚ 

ë””ìì´ë„ˆ: "ì¹´ë“œí‚¤ê°€ ì—†ë„¤ìš”?"
ê´€ë¦¬ì: "ì–¼êµ´ì´ ì¹´ë“œí‚¤ì˜ˆìš”. ë“±ë¡í•˜ì‹œì£ ."

[ì–¼êµ´ ë“±ë¡]
ğŸ“¸ ì •ë©´, ì¢Œì¸¡, ìš°ì¸¡ ì´¬ì˜
ì‹œìŠ¤í…œ: "ë“±ë¡ ì™„ë£Œ!"

[ë‹¤ìŒë‚  ì¶œê·¼]
ğŸ˜· ë§ˆìŠ¤í¬ ì°©ìš© â†’ âœ… ì¸ì‹ ì„±ê³µ
ğŸ•¶ ì„ ê¸€ë¼ìŠ¤ â†’ âœ… ì¸ì‹ ì„±ê³µ
ğŸ’„ ì§„í•œ í™”ì¥ â†’ âœ… ì¸ì‹ ì„±ê³µ

ë””ìì´ë„ˆ: "ì–´ë–»ê²Œ ë‹¤ ì•Œì•„ë³´ì£ ?"
ê°œë°œì: "128ì°¨ì› ë²¡í„°ë¡œ ì–¼êµ´ì„ ê¸°ì–µí•´ìš”"

[í•œ ë‹¬ í›„]
ì¶œì… ë¡œê·¸:
- ì •í™•ë„: 99.7%
- ì¸ì‹ ì‹œê°„: 0.3ì´ˆ
- ì˜¤ì¸ì‹: 0íšŒ

"SF ì˜í™”ê°€ í˜„ì‹¤ì´ ë˜ì—ˆë„¤ìš”"
```

**ì»´í“¨í„° ë¹„ì „ì€ í”½ì…€ì„ ì˜ë¯¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.**

## Part 1: ì»´í“¨í„° ë¹„ì „ ê¸°ì´ˆ

### ğŸ‘ ì´ë¯¸ì§€ ì´í•´í•˜ê¸°

```
ì»´í“¨í„°ê°€ ë³´ëŠ” ì´ë¯¸ì§€:

ì‚¬ëŒì´ ë³´ëŠ” ê²ƒ: ğŸ± ê³ ì–‘ì´
ì»´í“¨í„°ê°€ ë³´ëŠ” ê²ƒ: 

[[[255, 128, 64],  [248, 130, 68],  ...],
 [[251, 125, 61],  [245, 127, 65],  ...],
 [[247, 122, 58],  [241, 124, 62],  ...],
 ...]

= ìˆ«ì ë°°ì—´ (Height Ã— Width Ã— Channels)

ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
1. ì…ë ¥: Raw í”½ì…€
2. ì „ì²˜ë¦¬: ì •ê·œí™”, ë¦¬ì‚¬ì´ì¦ˆ
3. íŠ¹ì§• ì¶”ì¶œ: ì—£ì§€, ëª¨ì–‘, í…ìŠ¤ì²˜
4. ë¶„ë¥˜/ê²€ì¶œ: ì¹´í…Œê³ ë¦¬ ê²°ì •
5. í›„ì²˜ë¦¬: ì‹ ë¢°ë„, í•„í„°ë§

ê¸°ë³¸ ì—°ì‚°:
# OpenCV ì˜ˆì œ
import cv2
import numpy as np

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('cat.jpg')

# ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# ì—£ì§€ ê²€ì¶œ (Canny)
edges = cv2.Canny(gray, 100, 200)

# ì–¼êµ´ ê²€ì¶œ (Haar Cascade)
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
faces = face_cascade.detectMultiScale(gray, 1.3, 5)

for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
```

### ğŸ¯ ì£¼ìš” íƒœìŠ¤í¬

```
ì»´í“¨í„° ë¹„ì „ì˜ 5ëŒ€ ê³¼ì œ:

1. Image Classification (ë¶„ë¥˜)
"ì´ ì´ë¯¸ì§€ëŠ” ë¬´ì—‡?"
Input: ì´ë¯¸ì§€
Output: ì¹´í…Œê³ ë¦¬ (ê³ ì–‘ì´, ê°œ, ìë™ì°¨...)

2. Object Detection (ê²€ì¶œ)
"ì–´ë””ì— ë¬´ì—‡ì´ ìˆë‚˜?"
Input: ì´ë¯¸ì§€
Output: ë°”ìš´ë”© ë°•ìŠ¤ + í´ë˜ìŠ¤

3. Segmentation (ë¶„í• )
"ê° í”½ì…€ì€ ë¬´ì—‡?"
- Semantic: ê°™ì€ í´ë˜ìŠ¤ êµ¬ë¶„ ì•ˆí•¨
- Instance: ê°œë³„ ê°ì²´ êµ¬ë¶„
- Panoptic: ë‘˜ ë‹¤

4. Pose Estimation (ìì„¸ ì¶”ì •)
"ì‚¬ëŒì˜ ê´€ì ˆ ìœ„ì¹˜ëŠ”?"
Input: ì‚¬ëŒ ì´ë¯¸ì§€
Output: 17ê°œ í‚¤í¬ì¸íŠ¸

5. Image Generation (ìƒì„±)
"ì„¤ëª…ëŒ€ë¡œ ì´ë¯¸ì§€ ë§Œë“¤ê¸°"
Input: í…ìŠ¤íŠ¸ ì„¤ëª…
Output: ìƒì„±ëœ ì´ë¯¸ì§€
```

## Part 2: ì–¼êµ´ ì¸ì‹ ê¸°ìˆ 

### ğŸ˜Š Face Recognition Pipeline

```
ì–¼êµ´ ì¸ì‹ 5ë‹¨ê³„:

1. Face Detection (ì–¼êµ´ ê²€ì¶œ)
HOG, Haar Cascade, MTCNN
â†’ ì–¼êµ´ ì˜ì—­ ì°¾ê¸°

2. Face Alignment (ì •ë ¬)
ëœë“œë§ˆí¬ ê²€ì¶œ (68 points)
â†’ ëˆˆ, ì½”, ì… ìœ„ì¹˜
â†’ ì •ë©´ ì •ë ¬

3. Feature Extraction (íŠ¹ì§• ì¶”ì¶œ)
Deep Learning (FaceNet, ArcFace)
â†’ 128/512ì°¨ì› ë²¡í„°

4. Face Matching (ë§¤ì¹­)
ì½”ì‚¬ì¸ ìœ ì‚¬ë„, L2 ê±°ë¦¬
ì„ê³„ê°’: 0.6 (ì¡°ì ˆ ê°€ëŠ¥)

5. Liveness Detection (ìƒì²´ ê°ì§€)
ì‚¬ì§„/ì˜ìƒ êµ¬ë¶„
ê¹œë¹¡ì„, ë¯¸ì†Œ, ê³ ê°œ ëŒë¦¬ê¸°

êµ¬í˜„ ì˜ˆì œ:
import face_recognition

# ì–¼êµ´ ì¸ì½”ë”©
known_image = face_recognition.load_image_file("me.jpg")
known_encoding = face_recognition.face_encodings(known_image)[0]

# ìƒˆ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ì°¾ê¸°
unknown_image = face_recognition.load_image_file("group.jpg")
face_locations = face_recognition.face_locations(unknown_image)
face_encodings = face_recognition.face_encodings(unknown_image, face_locations)

# ë§¤ì¹­
for face_encoding in face_encodings:
    matches = face_recognition.compare_faces([known_encoding], face_encoding)
    distance = face_recognition.face_distance([known_encoding], face_encoding)
    
    if matches[0] and distance[0] < 0.6:
        print("ë‚˜ë¥¼ ì°¾ì•˜ë‹¤!")
```

### ğŸ­ ì–¼êµ´ ì¸ì‹ì˜ ë„ì „ê³¼ì œ

```
ê¸°ìˆ ì  ê³¼ì œ:

1. ì¡°ëª… ë³€í™”
í•´ê²°: ë°ì´í„° ì¦ê°•, ì •ê·œí™”

2. ê°ë„ ë³€í™”
í•´ê²°: 3D ëª¨ë¸ë§, ë‹¤ê°ë„ í•™ìŠµ

3. ê°€ë¦¼ (ë§ˆìŠ¤í¬, ì„ ê¸€ë¼ìŠ¤)
í•´ê²°: ë¶€ë¶„ ë§¤ì¹­, ëˆˆ ì£¼ë³€ ì§‘ì¤‘

4. ë‚˜ì´ ë³€í™”
í•´ê²°: Age-invariant features

5. í‘œì • ë³€í™”
í•´ê²°: ì¤‘ë¦½ í‘œì • ì •ê·œí™”

ìœ¤ë¦¬ì  ë¬¸ì œ:

Privacy (í”„ë¼ì´ë²„ì‹œ):
- ë™ì˜ ì—†ëŠ” ìˆ˜ì§‘
- ëŒ€ëŸ‰ ê°ì‹œ
- ë°ì´í„° ìœ ì¶œ

Bias (í¸í–¥):
- ì¸ì¢…ë³„ ì •í™•ë„ ì°¨ì´
- ì„±ë³„ í¸í–¥
- ë‚˜ì´ í¸í–¥

Misuse (ì˜¤ìš©):
- Deepfake
- ì‹ ì› ë„ìš©
- ìŠ¤í† í‚¹

ê·œì œì™€ ëŒ€ì‘:
EU GDPR: ëª…ì‹œì  ë™ì˜ í•„ìš”
ì¤‘êµ­: ê³µê³µì¥ì†Œ ì–¼êµ´ì¸ì‹ ì œí•œ
ê¸°ì—…: Opt-in ì •ì±…
```

## Part 3: OCRê³¼ ë¬¸ì„œ ì¸ì‹

### ğŸ“ Optical Character Recognition

```
OCR íŒŒì´í”„ë¼ì¸:

1. ì „ì²˜ë¦¬
- ë…¸ì´ì¦ˆ ì œê±°
- Skew ë³´ì •
- ì´ì§„í™” (Binarization)

2. í…ìŠ¤íŠ¸ ì˜ì—­ ê²€ì¶œ
- EAST, CRAFT
- ë¬¸ë‹¨/ì¤„/ë‹¨ì–´ ë¶„ë¦¬

3. ë¬¸ì ì¸ì‹
- CNN + RNN + CTC
- Transformer (TrOCR)

4. í›„ì²˜ë¦¬
- ë§ì¶¤ë²• ê²€ì‚¬
- ë¬¸ë§¥ ë³´ì •
- í¬ë§· ë³µì›

ì‹¤ì „ í™œìš©:
# Tesseract OCR
import pytesseract
from PIL import Image

# ê¸°ë³¸ OCR
text = pytesseract.image_to_string(Image.open('document.png'), lang='kor')

# ìƒì„¸ ì •ë³´
data = pytesseract.image_to_data(Image.open('document.png'), output_type=pytesseract.Output.DICT)

# ë ˆì´ì•„ì›ƒ ë¶„ì„
for i, word in enumerate(data['text']):
    if word.strip():
        x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
        confidence = data['conf'][i]
        
        if confidence > 60:  # ì‹ ë¢°ë„ 60% ì´ìƒ
            print(f"{word}: ({x}, {y}) - Confidence: {confidence}%")

í™œìš© ì‚¬ë¡€:
1. ëª…í•¨ ìŠ¤ìº” â†’ CRM ìë™ ì…ë ¥
2. ì˜ìˆ˜ì¦ â†’ ê²½ë¹„ ì²˜ë¦¬
3. ë¬¸ì„œ â†’ ê²€ìƒ‰ ê°€ëŠ¥ PDF
4. ë²ˆì—­ ì•± â†’ ì‹¤ì‹œê°„ ë²ˆì—­
5. ì°¨ëŸ‰ ë²ˆí˜¸íŒ â†’ ì£¼ì°¨ ê´€ë¦¬
```

## Part 4: ì‹¤ì‹œê°„ ë¹„ì „ ì‘ìš©

### ğŸš— ììœ¨ì£¼í–‰ê³¼ ADAS

```
ììœ¨ì£¼í–‰ ë¹„ì „ ì‹œìŠ¤í…œ:

ì„¼ì„œ í“¨ì „:
- ì¹´ë©”ë¼: ìƒ‰ìƒ, í…ìŠ¤íŠ¸
- LiDAR: ê±°ë¦¬, 3D
- Radar: ì†ë„, ë‚ ì”¨ ë¬´ê´€

ì£¼ìš” ê¸°ëŠ¥:

1. Lane Detection (ì°¨ì„  ì¸ì‹)
- Hough Transform
- Deep Learning (LaneNet)
- ê³¡ì„  í”¼íŒ…

2. Object Tracking (ê°ì²´ ì¶”ì )
- YOLO + SORT
- ë³´í–‰ì, ì°¨ëŸ‰, ìì „ê±°
- ê¶¤ì  ì˜ˆì¸¡

3. Traffic Sign Recognition
- CNN ë¶„ë¥˜
- ì‹¤ì‹œê°„ ì²˜ë¦¬ (30 FPS)
- ë‹¤êµ­ì–´ ì§€ì›

4. Depth Estimation
- Monocular: ë‹¨ì¼ ì¹´ë©”ë¼
- Stereo: ì–‘ì•ˆ ì‹œì°¨
- ê±°ë¦¬ ê³„ì‚°

êµ¬í˜„ ì˜ˆì œ:
class AutonomousVision:
    def __init__(self):
        self.lane_detector = LaneDetector()
        self.object_detector = YOLOv8()
        self.tracker = DeepSORT()
        
    def process_frame(self, frame):
        # ì°¨ì„  ê²€ì¶œ
        lanes = self.lane_detector.detect(frame)
        
        # ê°ì²´ ê²€ì¶œ
        detections = self.object_detector.detect(frame)
        
        # ì¶”ì 
        tracks = self.tracker.update(detections)
        
        # ìœ„í—˜ íŒë‹¨
        alerts = self.assess_risk(lanes, tracks)
        
        return {
            'lanes': lanes,
            'objects': tracks,
            'alerts': alerts
        }
```

### ğŸ“± AR/VR ì‘ìš©

```
ì¦ê°•í˜„ì‹¤ ì»´í“¨í„° ë¹„ì „:

1. SLAM (Simultaneous Localization and Mapping)
- ê³µê°„ ë§¤í•‘
- ìê¸° ìœ„ì¹˜ ì¶”ì 
- 3D ì¬êµ¬ì„±

2. Marker Detection
- QR/ArUco ë§ˆì»¤
- Natural Feature Tracking
- Image Target

3. Hand Tracking
- MediaPipe Hands
- 21ê°œ ëœë“œë§ˆí¬
- ì œìŠ¤ì²˜ ì¸ì‹

4. Face Filters
- ì–¼êµ´ ëœë“œë§ˆí¬
- 3D ë©”ì‹œ í”¼íŒ…
- ì‹¤ì‹œê°„ ë Œë”ë§

ARCore/ARKit í™œìš©:
// Swift - ARKit
func renderer(_ renderer: SCNSceneRenderer, nodeFor anchor: ARAnchor) -> SCNNode? {
    guard let faceAnchor = anchor as? ARFaceAnchor else { return nil }
    
    // ì–¼êµ´ ë©”ì‹œ
    let geometry = ARSCNFaceGeometry(device: device)
    geometry?.update(from: faceAnchor.geometry)
    
    // í•„í„° ì ìš©
    let node = SCNNode(geometry: geometry)
    node.geometry?.materials = [catEarsMaterial]
    
    return node
}
```

## Part 5: ë””ìì¸ ë„êµ¬ì˜ AI ë¹„ì „

### ğŸ¨ AI ê¸°ë°˜ ë””ìì¸ ë„êµ¬

```
ì»´í“¨í„° ë¹„ì „ in ë””ìì¸:

1. ë°°ê²½ ì œê±°
- Segment Anything (SAM)
- UÂ²-Net
- 1í´ë¦­ ëˆ„ë¼

2. ìŠ¤íƒ€ì¼ ë³€í™˜
- Neural Style Transfer
- ì‚¬ì§„ â†’ ê·¸ë¦¼
- ë‚® â†’ ë°¤

3. ì´ë¯¸ì§€ í–¥ìƒ
- Super Resolution (ESRGAN)
- Denoising
- Colorization

4. ì½˜í…ì¸  ì¸ì‹ í¸ì§‘
- Content-Aware Fill
- Object Removal
- Sky Replacement

5. 3D from 2D
- NeRF (Neural Radiance Fields)
- ë‹¨ì¼ ì´ë¯¸ì§€ â†’ 3D ëª¨ë¸
- 360ë„ ë·° ìƒì„±

Figma í”ŒëŸ¬ê·¸ì¸ ì˜ˆì œ:
// Remove.bg API í™œìš©
async function removeBackground(imageBytes) {
    const response = await fetch('https://api.remove.bg/v1.0/removebg', {
        method: 'POST',
        headers: {
            'X-Api-Key': API_KEY,
        },
        body: imageBytes
    });
    
    return await response.arrayBuffer();
}

// Figmaì— ì ìš©
const newImage = await removeBackground(selectedImage);
const imageNode = figma.createImage(newImage);
selectedNode.fills = [{ type: 'IMAGE', imageHash: imageNode.hash }];
```

## ğŸ Bonus: Edge Computing

### âš¡ ì˜¨ë””ë°”ì´ìŠ¤ ë¹„ì „

```
Edge AI ì¥ì :
- ì§€ì—°ì‹œê°„: <10ms
- í”„ë¼ì´ë²„ì‹œ: ë¡œì»¬ ì²˜ë¦¬
- ì˜¤í”„ë¼ì¸: ì¸í„°ë„· ë¶ˆí•„ìš”
- ë¹„ìš©: ì„œë²„ ë¹„ìš© ì—†ìŒ

ëª¨ë°”ì¼ ìµœì í™”:
1. ëª¨ë¸ ê²½ëŸ‰í™”
- Quantization (INT8)
- Pruning
- Knowledge Distillation

2. í”„ë ˆì„ì›Œí¬
- TensorFlow Lite
- Core ML
- ONNX Runtime

3. í•˜ë“œì›¨ì–´ ê°€ì†
- Apple Neural Engine
- Qualcomm Hexagon
- Google Edge TPU

ì„±ëŠ¥ ë¹„êµ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Model     â”‚ Cloud â”‚  Edge  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency     â”‚ 200ms â”‚  20ms  â”‚
â”‚ Accuracy    â”‚  95%  â”‚  92%   â”‚
â”‚ Model Size  â”‚ 500MB â”‚  10MB  â”‚
â”‚ Privacy     â”‚  Low  â”‚  High  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’¡ í•µì‹¬ ë©”ì‹œì§€

> "ì»´í“¨í„° ë¹„ì „ì€ ê¸°ê³„ì—ê²Œ ëˆˆì„ ì£¼ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
> í”½ì…€ì˜ ë‚˜ì—´ì—ì„œ ì˜ë¯¸ë¥¼ ì°¾ì•„ë‚´ê³ ,
> ë³´ëŠ” ê²ƒì„ ì´í•´í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
> 
> ì´ì œ ì»´í“¨í„°ëŠ” ë³´ê³ , ì´í•´í•˜ê³ , í–‰ë™í•©ë‹ˆë‹¤.
> ë””ìì´ë„ˆì˜ ì°½ì˜ì„±ê³¼ AIì˜ ì¸ì‹ì´ ë§Œë‚˜ë©´
> ë¬´í•œí•œ ê°€ëŠ¥ì„±ì´ ì—´ë¦½ë‹ˆë‹¤."

**ê¸°ì–µí•˜ì„¸ìš”:**
- í”½ì…€ â†’ íŠ¹ì§• â†’ ì˜ë¯¸
- ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
- í”„ë¼ì´ë²„ì‹œ ìš°ì„ 
- Edge > Cloud (ê°€ëŠ¥í•˜ë©´)

## ğŸš€ ë‹¤ìŒ ì—í”¼ì†Œë“œ ì˜ˆê³ 

**"Episode 10-4: ìì—°ì–´ ì²˜ë¦¬, AIê°€ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë²•"**

í…ìŠ¤íŠ¸ì˜ ë§ˆë²•:
- í† í°í™”ì™€ ì„ë² ë”©
- ê°ì • ë¶„ì„
- ê¸°ê³„ ë²ˆì—­
- ì±—ë´‡ ë§Œë“¤ê¸°

"ì»´í“¨í„°ëŠ” ì–´ë–»ê²Œ ë§ì„ ì´í•´í• ê¹Œ?"

---

*"The eye sees only what the mind is prepared to comprehend."
- Robertson Davies*

**#ì»´í“¨í„°ë¹„ì „ #OpenCV #ì–¼êµ´ì¸ì‹ #OCR #ObjectDetection**